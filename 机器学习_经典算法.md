**RoadMap**
---
- [决策树](#决策树)

**Index**
---
<!-- TOC -->
- [决策树](#决策树)
    - [信息增益与信息增益比 TODO](#信息增益与信息增益比-todo)
    - [分类树 - ID3 决策树与 C4.5 决策树 TODO](#分类树---id3-决策树与-c45-决策树-todo)
    - [决策树如何避免过拟合 TODO](#决策树如何避免过拟合-todo)
    - [回归树 - CART 决策树](#回归树---cart-决策树)
        - [CART 回归树算法推导](#cart-回归树算法推导)
        - [示例: 选择切分变量与切分点](#示例-选择切分变量与切分点)
        
 # 决策树
- 决策树的训练通常由三部分组成：**特征选择**、**树的生成**、**剪枝**。

## 信息增益与信息增益比 TODO
## 分类树 - ID3 决策树与 C4.5 决策树 TODO
- ID3 决策树和 C4.5 决策树的**区别**在于：前者使用**信息增益**来进行特征选择，而后者使用**信息增益比**。

## 决策树如何避免过拟合 TODO

## 回归树 - CART 决策树
> 《统计学习方法》 5.5 CART 算法
- CART 算法是在给定输入随机变量 _`X`_ 条件下输出随机变量 _`Y`_ 的**条件概率分布**的学习方法。 
- CART 算法假设决策树是**二叉树**，内部节点特征的取值为“**是**”和“**否**”。

  这样的决策树等价于递归地二分每个特征，**将输入空间/特征空间划分为有限个单元**，然后在这些单元上确定在输入给定的条件下输出的**条件概率分布**。
- CART 决策树**既可以用于分类，也可以用于回归**；

  对回归树 CART 算法用**平方误差最小化**准则来选择特征，对分类树用**基尼指数最小化**准则选择特征

